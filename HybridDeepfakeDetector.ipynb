{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anaaaraujoo/deepfake-detection/blob/main/HybridDeepfakeDetector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty2inJ0Xei-z"
      },
      "source": [
        "### **Hybrid deepfake detection (CNN-LSTM-Transformer Approach)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "d3cdTSO62BYH",
        "outputId": "72631ef7-65f6-49d6-a567-5e4f71440bb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m832.4/832.4 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m121.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.8/241.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.4/224.4 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.6/355.6 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Installing all the necessary libraries and frameworks\n",
        "!pip install -q tensorflow matplotlib seaborn scikit-learn opencv-python numpy pandas tqdm pytorch-lightning timm gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Austj_lP6my2"
      },
      "outputs": [],
      "source": [
        "# Importing all the necessary libraries\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torchvision\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "from tensorflow.keras.layers import Input, TimeDistributed, Dense, LSTM, Dropout, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hl6JWOG16AKR"
      },
      "outputs": [],
      "source": [
        "# Downloading and unziping the dataset - CelebDF\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "drive.mount('/content/drive')\n",
        "dataset_path = '/content/drive/MyDrive/Celeb-DF.zip'\n",
        "extract_path = '/content'\n",
        "zip_ref = zipfile.ZipFile(dataset_path, 'r')\n",
        "zip_ref.extractall(extract_path)\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bbwhg3IXDH6Y"
      },
      "outputs": [],
      "source": [
        "# Data analysis\n",
        "# Plotting some samples for visual comprehension\n",
        "real_path = '/content/Celeb-real'\n",
        "yt_path = '/content/YouTube-real'\n",
        "fake_path = '/content/Celeb-synthesis'\n",
        "\n",
        "real_videos_samples = [os.path.join(real_path, f) for f in os.listdir(real_path) if f.endswith(\".mp4\")]\n",
        "fake_videos_samples = [os.path.join(fake_path, f) for f in os.listdir(fake_path) if f.endswith(\".mp4\")]\n",
        "yt_videos_samples = [os.path.join(yt_path, f) for f in os.listdir(yt_path) if f.endswith(\".mp4\")]\n",
        "\n",
        "print(f'Number of real videos on the training set: {len(real_videos_samples) + len(yt_videos_samples)}')\n",
        "print(f'Number of fake videos on the training set: {len(fake_videos_samples)}')\n",
        "\n",
        "n = 3 # Samples to plot per class\n",
        "\n",
        "def extract_first_frame(video_path):\n",
        "  cap = cv2.VideoCapture(video_path)\n",
        "  ret, frame = cap.read()\n",
        "  cap.release()\n",
        "  return frame\n",
        "\n",
        "i = 1\n",
        "print('Real Samples: ')\n",
        "plt.rcParams['figure.figsize'] = [9, 9]\n",
        "for video in real_videos_samples[:n]:\n",
        "  ax = plt.subplot(3, n, i)\n",
        "  frame = extract_first_frame(video)\n",
        "  plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "  i += 1\n",
        "plt.show()\n",
        "\n",
        "print('Fake Samples: ')\n",
        "plt.rcParams['figure.figsize'] = [9, 9]\n",
        "for video in fake_videos_samples[:n]:\n",
        "  ax = plt.subplot(3, n, i)\n",
        "  frame = extract_first_frame(video)\n",
        "  plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "  i += 1\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MRBfGUQVDfs"
      },
      "outputs": [],
      "source": [
        "# Dataset class\n",
        "class CelebDF:\n",
        "  def __init__(self, video_paths, labels, detect_faces_flag=True, max_frames=15, img_size = 112):\n",
        "    self.videos = video_paths\n",
        "    self.labels = labels\n",
        "    self.detect_faces_flag = detect_faces_flag\n",
        "    self.img_size = img_size\n",
        "    self.max_frames = max_frames\n",
        "\n",
        "    haar_cascade_url = 'https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml'\n",
        "    haar_cascade_path = 'haarcascade_frontalface_default.xml'\n",
        "    if not os.path.exists(haar_cascade_path):\n",
        "        import urllib.request\n",
        "        urllib.request.urlretrieve(haar_cascade_url, haar_cascade_path)\n",
        "    self.face_cascade = cv2.CascadeClassifier(haar_cascade_path)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.videos)\n",
        "\n",
        "  # Frame extractor\n",
        "  def frame_extractor(self, video_path):\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "      print(f'Error opening video file: {video_path}')\n",
        "      return np.zeros((self.max_frames, self.img_size, self.img_size, 3), dtype=np.float32)\n",
        "    frame_count = 0\n",
        "    while True:\n",
        "      ret, frame = cap.read()\n",
        "      if not ret or frame_count >= self.max_frames:\n",
        "        break\n",
        "      frame = cv2.resize(frame, (224, 224))\n",
        "      frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "      frames.append(frame)\n",
        "      frame_count += 1\n",
        "    cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "  # Face recognition\n",
        "  def detect_faces(self, frames):\n",
        "    processed_frames = []\n",
        "    for frame in frames:\n",
        "      gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "      faces = self.face_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "      if len(faces) > 0:\n",
        "        largest_face = max(faces, key=lambda x: x[2] * x[3])\n",
        "        x, y, w, h = largest_face\n",
        "        face = cv2.resize(frame[y:y+h, x:x+w], (224, 224))\n",
        "        processed_frames.append(face)\n",
        "      else:\n",
        "        processed_frames.append(np.zeros((224, 224, 3), dtype=np.uint8))\n",
        "    return np.array(processed_frames) if processed_frames else np.array([])\n",
        "\n",
        "  # Each output is in TensorFlow format\n",
        "  def __getitem__(self, idx):\n",
        "    video_path = self.videos[idx]\n",
        "    label = self.labels[idx]\n",
        "    frames = self.frame_extractor(str(video_path))\n",
        "    if (len(frames) == 0):\n",
        "      return tf.zeros((1, 224, 224, 3), dtype=tf.float32), tf.constant(0, dtype=tf.int32)\n",
        "\n",
        "    if self.crop_faces_flag:\n",
        "      frames_crops = self.detect_faces(frames)\n",
        "      if frames_crops.size > 0:\n",
        "        frames = frames_crops\n",
        "      else:\n",
        "        return tf.zeros((1, 3, 224, 224), dtype=tf.float32), tf.constant(0, dtype=tf.int32)\n",
        "\n",
        "    frames = tf.convert_to_tensor(frames, dtype=tf.float32) / 255.0\n",
        "    label = tf.constant(1 if \"real\" in str(video_path) else 0, dtype=tf.int32)\n",
        "    return frames, label\n",
        "\n",
        "  def generator(self):\n",
        "    for video_path in self.videos:\n",
        "      frames = self.frame_extractor(str(video_path))\n",
        "      if (len(frames) == 0):\n",
        "        yield tf.zeros((1, 224, 224, 3), dtype=tf.float32), tf.constant(0, dtype=tf.int32)\n",
        "        continue\n",
        "      if self.crop_faces_flag:\n",
        "        frames_crops = self.detect_faces(frames)\n",
        "        if frames_crops.size > 0:\n",
        "          frames = frames_crops\n",
        "        else:\n",
        "          yield tf.zeros((1, 3, 224, 224), dtype=tf.float32), tf.constant(0, dtype=tf.int32)\n",
        "          continue\n",
        "      frames = tf.convert_to_tensor(frames, dtype=tf.float32) / 255.0\n",
        "      label = tf.constant(1 if \"real\" in str(video_path) else 0, dtype=tf.int32)\n",
        "      yield frames, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7bRywy8yUbt"
      },
      "outputs": [],
      "source": [
        "# Dividing the data in the train/val/test split\n",
        "# Extracting videos from the .txt file\n",
        "def read_split_list(txt_file):\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        videos = [line.strip() for line in f.readlines()]\n",
        "    return videos\n",
        "\n",
        "real_videos_root = Path(\"/content/Celeb-real\")\n",
        "yt_videos_root = Path(\"/content/YouTube-real\")\n",
        "fake_videos_root = Path(\"/content/Celeb-synthesis\")\n",
        "test_videos_root = Path(\"/content/List_of_testing_videos.txt\")\n",
        "\n",
        "real_videos = list(real_videos_root.rglob(\"*.mp4\"))\n",
        "yt_videos = list(yt_videos_root.rglob(\"*.mp4\"))\n",
        "fake_videos = list(fake_videos_root.rglob(\"*.mp4\"))\n",
        "test_videos  = read_split_list(test_videos_root)\n",
        "\n",
        "all_videos = real_videos + fake_videos + yt_videos\n",
        "print(f\"Total number of videos: {len(all_videos)}\")\n",
        "\n",
        "# Create labels for the videos: 0 -> real, 1 -> fake\n",
        "train_labels = [1] * (len(real_videos) + len(yt_videos)) + [0] * len(fake_videos)\n",
        "\n",
        "train_videos, val_videos, train_labels, val_labels = train_test_split(\n",
        "    all_videos, train_labels,\n",
        "    test_size = 0.15,\n",
        "    random_state = 42,\n",
        "    stratify = train_labels\n",
        ")\n",
        "\n",
        "train_data = CelebDF(train_videos, detect_faces_flag=True)\n",
        "val_data   = CelebDF(val_videos, detect_faces_flag=True)\n",
        "test_data  = CelebDF(test_videos, detect_faces_flag=True)\n",
        "\n",
        "train_ds = tf.data.Dataset.from_generator(\n",
        "    train_data.generator,\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(), dtype=tf.int32),\n",
        "    )\n",
        ").padded_batch(4).shuffle(100).cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_generator(\n",
        "    val_data.generator,\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(), dtype=tf.int32),\n",
        "    )\n",
        ").padded_batch(4).cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_generator(\n",
        "    test_data.generator,\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(), dtype=tf.int32),\n",
        "    )\n",
        ").padded_batch(4).cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(f\"Number of training videos: {len(train_data)}\")\n",
        "print(f\"Number of validation videos: {len(val_data)}\")\n",
        "print(f\"Number of testing videos: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG-yEBPUUHG0"
      },
      "outputs": [],
      "source": [
        "# Testing the frame extractor and the face recognition functions\n",
        "max_display = 5\n",
        "frames, label = train_data[1]\n",
        "print(f'Video; {train_data.videos[1]}')\n",
        "print(f'Label: {label}')\n",
        "print(f'Frames: {frames.shape}')\n",
        "fig, axes = plt.subplots(1, max_display, figsize=(4 * max_display, 4))\n",
        "for i in range (max_display):\n",
        "  axes[i].imshow(frames[i].numpy().astype('float32')) # Convert to float32 for display\n",
        "  axes[i].set_title(f'Frame nº{i+1}')\n",
        "  axes[i].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tucIO8bS0Gy0"
      },
      "outputs": [],
      "source": [
        "# CNN-LSTM Model\n",
        "# The Xception model only outputs features, that is why the fully connected top layer (that\n",
        "# predicts classes scores) is left out\n",
        "def cnn_lstm_model():\n",
        "  cnn_backbone = tf.keras.applications.MobileNetV2(\n",
        "      weights='imagenet',\n",
        "      include_top=False,\n",
        "      input_shape=(img_size, img_size, 3)\n",
        "  )\n",
        "\n",
        "  for layer in cnn_backbone.layers[:-20]:\n",
        "    layer.trainable = False # Some layers are not trainable to reduce memory usage\n",
        "  video_input = Input(shape=(max_frames, img_size, img_size, 3))\n",
        "  cnn_features = TimeDistributed(cnn_backbone)(video_input)\n",
        "  cnn_features = TimeDistributed(GlobalAveragePooling2D())(cnn_features)\n",
        "  cnn_features = (Dropout(0.3))(cnn_features)\n",
        "\n",
        "  lstm_out = LSTM(64, return_sequences)(cnn_features)\n",
        "  lstm_out = Dropout(0.4)(lstm_out)\n",
        "  lstm_out = LSTM(32)(lstm_out)\n",
        "  lstm_features = Dropout(0.4)(lstm_out)\n",
        "\n",
        "  output = Dense(1, activation='sigmoid', dtype='float32')(lstm_features)\n",
        "  model = Model(inputs=video_input, outputs=output)\n",
        "  optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAq4K7WXcF5k"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "model = cnn_lstm_model()\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    epochs = 10,\n",
        "    validation_data = val_ds,\n",
        "    verbose = 1 # Progress bar\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "authorship_tag": "ABX9TyPXFT5oD8p60p1NtZk/NmcM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}